—------------------------------------------------------------------------------------------------------------------------------
# Gold standard youtubers:
https://www.youtube.com/@umarjamilai/videos
https://www.youtube.com/@AndrejKarpathy
https://www.youtube.com/@YannicKilcher


-------------------------------------------------------------------------------------------------------------------------------
# Communities

https://www.lesswrong.com/

These two below have active discords:
https://www.eleuther.ai/
https://www.youtube.com/@YannicKilcher


—------------------------------------------------------------------------------------------------------------------------------
# Tools for better workflows / projectsd

AI workflow for lots of code output:

Cursor:
https://www.youtube.com/watch?v=1kPr1vy0-QY
https://www.youtube.com/watch?v=yk9lXobJ95E
https://www.youtube.com/watch?v=ksJB3x6hiWU
https://www.youtube.com/watch?v=gqUQbjsYZLQ
https://www.youtube.com/watch?v=Vy7dJKv1EpA
https://cursor.directory/
https://github.com/PatrickJS/awesome-cursorrules


ChainForge:
https://www.youtube.com/watch?v=iHWwxy8HFW4
https://www.chainforge.ai/


Docker:
https://www.youtube.com/watch?v=z12vR9In_eE
https://www.youtube.com/watch?v=X7guekGZM20
https://www.youtube.com/watch?v=b1RavPr_878

https://towardsdatascience.com/build-and-run-a-docker-container-for-your-machine-learning-model-60209c2d7a7f
https://medium.com/@preeti.rana.ai/ml-model-deployment-on-docker-de8ed92f852f
https://github.com/saikhu/Docker-Guide-for-AI-Model-Development-and-Deployment
https://www.youtube.com/watch?v=iX0HbrfRyvc
https://www.youtube.com/watch?v=zQVYjmGFvv0
https://www.youtube.com/watch?v=yPuhGtJT55o

https://www.youtube.com/watch?v=AhPXGKG4RZ4

https://www.youtube.com/watch?v=OpmMe0md0tA



—-----------------------------------------------------------------
# Scaling laws:
https://www.youtube.com/watch?v=5eqRuVp65eY
https://arxiv.org/pdf/2203.15556 Chinchilla scaling laws
https://arxiv.org/pdf/2102.06701 Explaining Neural Scaling Laws
https://arxiv.org/pdf/2104.04473 Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM 

https://arxiv.org/pdf/2411.04330 Scaling laws for precision

—---------------------------------------------------------------------------------------------
# (PEFT) Parameter efficient Fine Tuning:

LoRA:
https://arxiv.org/abs/2012.13255 intrinsic dimensionality 
https://arxiv.org/abs/1804.08838  measuring intrinsic dimensionality 
https://arxiv.org/pdf/2106.09685 LoRA
https://arxiv.org/abs/2305.14314 QLoRA
https://arxiv.org/pdf/2405.09673 LoRA Learns Less and Forgets Less
https://arxiv.org/pdf/2410.20672 layerwise LoRA


RLHF:
Todo
https://arxiv.org/pdf/2305.11206 LIMA

DPO: Direct Preference Optimization
https://arxiv.org/pdf/2305.18290



—---------------------------------------------------------------------------------------------
# Modern LLMs:

https://huggingface.co/deepseek-ai/DeepSeek-V2
https://arxiv.org/pdf/2404.16821 InternVL
https://arxiv.org/pdf/2404.06512 InternLM-XComposer2-4KHD
https://arxiv.org/pdf/2406.16860 Cambrian
https://arxiv.org/pdf/2403.18814 Mini-gemini: Mining the potential of multi-modality vision language models.
https://arxiv.org/pdf/2403.15388 LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models 
https://arxiv.org/abs/2411.04996 Mixture-of-Transformers

https://arxiv.org/pdf/2410.09918v1 Dualformer

https://github.com/PKU-YuanGroup/LLaVA-o1
https://arxiv.org/pdf/2312.06109

SigLIP family serving as visual encoders for LVLMs
https://arxiv.org/pdf/2402.11530 Efficient multimodal learning from data-centric perspective. 
https://arxiv.org/pdf/2405.02246 What matters when building vision-language models?

https://arxiv.org/pdf/2407.10671 Qwen2
https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md llama3

https://github.com/GAIR-NLP/O1-Journey ChatGPT 01 replication

Self alignment:
https://arxiv.org/pdf/2410.24198

Memorization and reasoning:
https://arxiv.org/pdf/2410.23123

Interpreting Language models:
https://arxiv.org/pdf/2411.05037

https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/

https://blog.bagel.net/p/train-fast-but-think-slow Train fast think slow


Monte Carlo Tree search
https://arxiv.org/pdf/2406.07394
https://arxiv.org/pdf/2410.02884
https://github.com/trotsky1997/MathBlackBox


https://arxiv.org/abs/2410.22113 Large learning rates
—---------------------------------------------------------------------------------------------

RAG:
https://arxiv.org/pdf/2405.13792 xrag

Text compression
https://arxiv.org/pdf/2307.06945  IN-CONTEXT AUTOENCODER FOR CONTEXT COMPRESSION IN A LARGE LANGUAGE MODEL

—---------------------------------------------------------------------------------------------
# Agents:

LLM agents berkeley START HERE
https://llmagents-learning.org/f24 
https://www.youtube.com/playlist?list=PLS01nW3RtgopsNLeM936V4TNSsvvVglLc

https://jina.ai/news/scaling-test-time-compute-for-embedding-models/

https://leehanchung.github.io/blogs/2024/10/26/thoughts-on-agents/

https://multion.ai/blog/introducing-agent-q-research-breakthrough-for-the-next-generation-of-ai-agents-with-planning-and-self-healing-capabilities

https://www.langchain.com/stateofaiagents

https://github.com/THUDM/WebRL/tree/main webRL
https://github.com/THUDM/VisualAgentBench/tree/main Visual agent benchmark


https://github.com/stitionai/devika Devika

https://lab42.global/community-model-efficiency/ Model efficiency for agents

https://arxiv.org/abs/2412.06769 Coconut

More resources:
https://github.com/samkhur006/awesome-llm-planning-reasoning
https://github.com/hijkzzz/Awesome-LLM-Strawberry
—---------------------------------------------------------------------------------------------

# Distributed Training:

https://github.com/NousResearch/DisTrO  DisTrO

https://github.com/learning-at-home/hivemind hivemind

https://app.primeintellect.ai/intelligence


—--------------------------------------------------------------------------------------------------------------------------------
# Diffusion:

https://arxiv.org/pdf/2112.10752 stable diffusion

https://arxiv.org/abs/2410.02543v2 diffusion as evolutionary algorithms
https://arxiv.org/abs/2211.01364 
—-----------------------------------------------------------------------------------------------------------------------------------------------------------------
# Other:

https://github.com/Wenyueh/game_theory

https://adamdad.github.io/neumeta/

https://arxiv.org/abs/2203.04794

https://cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization muP

https://semianalysis.com/2023/01/24/the-ai-brick-wall-a-practical-limit/

